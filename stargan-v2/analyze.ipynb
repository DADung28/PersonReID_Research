{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from munch import Munch\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "\n",
    "from core.data_loader import get_train_loader\n",
    "from core.data_loader import get_test_loader\n",
    "from core.solver import Solver\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model arguments\n",
    "parser.add_argument('--img_size', type=int, default=256,\n",
    "                    help='Image resolution')\n",
    "parser.add_argument('--num_domains', type=int, default=3,\n",
    "                    help='Number of domains')\n",
    "parser.add_argument('--latent_dim', type=int, default=16,\n",
    "                    help='Latent vector dimension')\n",
    "parser.add_argument('--hidden_dim', type=int, default=512,\n",
    "                    help='Hidden dimension of mapping network')\n",
    "parser.add_argument('--style_dim', type=int, default=64,\n",
    "                    help='Style code dimension')\n",
    "\n",
    "# weight for objective functions\n",
    "parser.add_argument('--lambda_reg', type=float, default=1,\n",
    "                    help='Weight for R1 regularization')\n",
    "parser.add_argument('--lambda_cyc', type=float, default=1,\n",
    "                    help='Weight for cyclic consistency loss')\n",
    "parser.add_argument('--lambda_sty', type=float, default=1,\n",
    "                    help='Weight for style reconstruction loss')\n",
    "parser.add_argument('--lambda_ds', type=float, default=2,\n",
    "                    help='Weight for diversity sensitive loss')\n",
    "parser.add_argument('--ds_iter', type=int, default=100000,\n",
    "                    help='Number of iterations to optimize diversity sensitive loss')\n",
    "parser.add_argument('--w_hpf', type=float, default=0,\n",
    "                    help='weight for high-pass filtering')\n",
    "\n",
    "# training arguments\n",
    "parser.add_argument('--randcrop_prob', type=float, default=0.5,\n",
    "                    help='Probabilty of using random-resized cropping')\n",
    "parser.add_argument('--total_iters', type=int, default=100000,\n",
    "                    help='Number of total iterations')\n",
    "parser.add_argument('--resume_iter', type=int, default=0,\n",
    "                    help='Iterations to resume training/testing')\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "                    help='Batch size for training')\n",
    "parser.add_argument('--val_batch_size', type=int, default=32,\n",
    "                    help='Batch size for validation')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='Learning rate for D, E and G')\n",
    "parser.add_argument('--f_lr', type=float, default=1e-6,\n",
    "                    help='Learning rate for F')\n",
    "parser.add_argument('--beta1', type=float, default=0.0,\n",
    "                    help='Decay rate for 1st moment of Adam')\n",
    "parser.add_argument('--beta2', type=float, default=0.99,\n",
    "                    help='Decay rate for 2nd moment of Adam')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='Weight decay for optimizer')\n",
    "parser.add_argument('--num_outs_per_domain', type=int, default=10,\n",
    "                    help='Number of generated images per domain during sampling')\n",
    "\n",
    "# misc\n",
    "parser.add_argument('--mode', type=str, required=True, default = 'train',\n",
    "                    choices=['train', 'sample', 'eval', 'align'],\n",
    "                    help='This argument is used in solver')\n",
    "parser.add_argument('--num_workers', type=int, default=4,\n",
    "                    help='Number of workers used in DataLoader')\n",
    "parser.add_argument('--seed', type=int, default=777,\n",
    "                    help='Seed for random number generator')\n",
    "\n",
    "# directory for training\n",
    "parser.add_argument('--train_img_dir', type=str, default='data/celeba_hq/train',\n",
    "                    help='Directory containing training images')\n",
    "parser.add_argument('--val_img_dir', type=str, default='data/celeba_hq/val',\n",
    "                    help='Directory containing validation images')\n",
    "parser.add_argument('--sample_dir', type=str, default='/home/jun/stargan-v2/jikken/test/samples',\n",
    "                    help='Directory for saving generated images')\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='/home/jun/stargan-v2/jikken/test/checkpoints',\n",
    "                    help='Directory for saving network checkpoints')\n",
    "\n",
    "# directory for calculating metrics\n",
    "parser.add_argument('--eval_dir', type=str, default='expr/eval',\n",
    "                    help='Directory for saving metrics, i.e., FID and LPIPS')\n",
    "\n",
    "# directory for testing\n",
    "parser.add_argument('--result_dir', type=str, default='expr/results',\n",
    "                    help='Directory for saving generated images and videos')\n",
    "parser.add_argument('--src_dir', type=str, default='assets/representative/celeba_hq/src',\n",
    "                    help='Directory containing input source images')\n",
    "parser.add_argument('--ref_dir', type=str, default='assets/representative/celeba_hq/ref',\n",
    "                    help='Directory containing input reference images')\n",
    "parser.add_argument('--inp_dir', type=str, default='assets/representative/custom/female',\n",
    "                    help='input directory when aligning faces')\n",
    "parser.add_argument('--out_dir', type=str, default='assets/representative/celeba_hq/src/female',\n",
    "                    help='output directory when aligning faces')\n",
    "\n",
    "# face alignment\n",
    "parser.add_argument('--wing_path', type=str, default='/home/jun/stargan-v2/expr/checkpoints/wing.ckpt')\n",
    "parser.add_argument('--lm_path', type=str, default='expr/checkpoints/celeba_lm_mean.npz')\n",
    "\n",
    "# step size\n",
    "parser.add_argument('--print_every', type=int, default=10)\n",
    "parser.add_argument('--sample_every', type=int, default=5000)\n",
    "parser.add_argument('--save_every', type=int, default=10000)\n",
    "parser.add_argument('--eval_every', type=int, default=50000)\n",
    "\n",
    "args = parser.parse_args(['--mode','train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "\n",
    "def subdirs(dname):\n",
    "    return [d for d in os.listdir(dname)\n",
    "            if os.path.isdir(os.path.join(dname, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(img_size=256, num_domains=2, latent_dim=16, hidden_dim=512, style_dim=64, lambda_reg=1, lambda_cyc=1, lambda_sty=1, lambda_ds=1, ds_iter=100000, w_hpf=1, randcrop_prob=0.5, total_iters=100000, resume_iter=0, batch_size=8, val_batch_size=32, lr=0.0001, f_lr=1e-06, beta1=0.0, beta2=0.99, weight_decay=0.0001, num_outs_per_domain=10, mode='train', num_workers=4, seed=777, train_img_dir='data/celeba_hq/train', val_img_dir='data/celeba_hq/val', sample_dir='/home/jun/stargan-v2/jikken/test/samples', checkpoint_dir='/home/jun/stargan-v2/jikken/test/checkpoints', eval_dir='expr/eval', result_dir='expr/results', src_dir='assets/representative/celeba_hq/src', ref_dir='assets/representative/celeba_hq/ref', inp_dir='assets/representative/custom/female', out_dir='assets/representative/celeba_hq/src/female', wing_path='expr/checkpoints/wing.ckpt', lm_path='expr/checkpoints/celeba_lm_mean.npz', print_every=10, sample_every=5000, save_every=10000, eval_every=50000)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of generator: 33892995\n",
      "Number of parameters of mapping_network: 3259072\n",
      "Number of parameters of style_encoder: 20949760\n",
      "Number of parameters of discriminator: 20852803\n",
      "Initializing generator...\n",
      "Initializing mapping_network...\n",
      "Initializing style_encoder...\n",
      "Initializing discriminator...\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "solver = Solver(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataLoader to fetch source images during the training phase...\n",
      "Preparing DataLoader to fetch reference images during the training phase...\n",
      "Preparing DataLoader for the generation phase...\n"
     ]
    }
   ],
   "source": [
    "loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='source',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers),\n",
    "                        ref=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='reference',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers),\n",
    "                        val=get_test_loader(root=args.val_img_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'src': <torch.utils.data.dataloader.DataLoader object at 0x7b886c193fe0>, 'ref': <torch.utils.data.dataloader.DataLoader object at 0x7b88567f7dd0>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7b88501d58e0>})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7b886c193fe0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7b88567f7dd0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7b88501d58e0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders['src'], loaders['ref'], loaders['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 256, 256]), torch.Size([8]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(loaders['src']))\n",
    "data[0].shape, data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 256, 256]), torch.Size([8]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(loaders['ref']))\n",
    "data[0].shape, data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
