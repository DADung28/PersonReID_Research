{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import save_images\n",
    "from util import html\n",
    "import util.util as util\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import models\n",
    "import data\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.CUT_mode = 'CUT'\n",
    "        self.batch_size = 1\n",
    "        self.checkpoints_dir = './checkpoints'\n",
    "        self.crop_size = 256\n",
    "        self.dataroot = '/home/jun/CUT/datasets/market2duke'\n",
    "        self.dataset_mode = 'unaligned'\n",
    "        self.direction = 'AtoB'\n",
    "        self.display_winsize = 256\n",
    "        self.easy_label = 'market2duke'\n",
    "        self.epoch = '10'\n",
    "        self.eval = False\n",
    "        self.flip_equivariance = False\n",
    "        self.gpu_ids = [0]\n",
    "        self.init_gain = 0.02\n",
    "        self.init_type = 'xavier'\n",
    "        self.input_nc = 3\n",
    "        self.isTrain = False\n",
    "        self.lambda_GAN = 1.0\n",
    "        self.lambda_NCE = 1.0\n",
    "        self.load_size = 256\n",
    "        self.max_dataset_size = float(\"inf\")\n",
    "        self.model = 'cut'\n",
    "        self.n_layers_D = 3\n",
    "        self.name = 'market2duke_segcut'\n",
    "        self.nce_T = 0.07\n",
    "        self.nce_idt = True\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.nce_layers = '0,4,8,12,16'\n",
    "        self.ndf = 64\n",
    "        self.netD = 'basic'\n",
    "        self.netF = 'mlp_sample'\n",
    "        self.netF_nc = 256\n",
    "        self.netG = 'resnet_9blocks'\n",
    "        self.ngf = 64\n",
    "        self.no_antialias = False\n",
    "        self.no_antialias_up = False\n",
    "        self.no_dropout = True\n",
    "        self.no_flip = False\n",
    "        self.normD = 'instance'\n",
    "        self.normG = 'instance'\n",
    "        self.num_patches = 256\n",
    "        self.num_test = 50\n",
    "        self.num_threads = 4\n",
    "        self.output_nc = 3\n",
    "        self.phase = 'test'\n",
    "        self.pool_size = 0\n",
    "        self.preprocess = 'resize_and_crop'\n",
    "        self.random_scale_max = 3.0\n",
    "        self.results_dir = './results/'\n",
    "        self.serial_batches = False\n",
    "        self.stylegan2_G_num_downsampling = 1\n",
    "        self.suffix = ''\n",
    "        self.verbose = False\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model [CUTModel] was created\n",
      "loading the model from ./checkpoints/duke2market_spcut/latest_net_G.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import save_images\n",
    "from util import html\n",
    "import util.util as util\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import models\n",
    "import data\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.CUT_mode = 'CUT'\n",
    "        self.batch_size = 1\n",
    "        self.checkpoints_dir = './checkpoints'\n",
    "        self.crop_size = 256\n",
    "        self.dataroot = '/home/jun/CUT/datasets/market2duke'\n",
    "        self.dataset_mode = 'unaligned'\n",
    "        self.direction = 'AtoB'\n",
    "        self.display_winsize = 256\n",
    "        self.easy_label = 'market2duke'\n",
    "        self.epoch = '10'\n",
    "        self.eval = False\n",
    "        self.flip_equivariance = False\n",
    "        self.gpu_ids = [0]\n",
    "        self.init_gain = 0.02\n",
    "        self.init_type = 'xavier'\n",
    "        self.input_nc = 3\n",
    "        self.isTrain = False\n",
    "        self.lambda_GAN = 1.0\n",
    "        self.lambda_NCE = 1.0\n",
    "        self.load_size = 256\n",
    "        self.max_dataset_size = float(\"inf\")\n",
    "        self.model = 'cut'\n",
    "        self.n_layers_D = 3\n",
    "        self.name = 'duke2market_spcut'\n",
    "        self.nce_T = 0.07\n",
    "        self.nce_idt = True\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.nce_layers = '0,4,8,12,16'\n",
    "        self.ndf = 64\n",
    "        self.netD = 'basic'\n",
    "        self.netF = 'mlp_sample'\n",
    "        self.netF_nc = 256\n",
    "        self.netG = 'resnet_9blocks'\n",
    "        self.ngf = 64\n",
    "        self.no_antialias = False\n",
    "        self.no_antialias_up = False\n",
    "        self.no_dropout = True\n",
    "        self.no_flip = False\n",
    "        self.normD = 'instance'\n",
    "        self.normG = 'instance'\n",
    "        self.num_patches = 256\n",
    "        self.num_test = 50\n",
    "        self.num_threads = 4\n",
    "        self.output_nc = 3\n",
    "        self.phase = 'test'\n",
    "        self.pool_size = 0\n",
    "        self.preprocess = 'resize_and_crop'\n",
    "        self.random_scale_max = 3.0\n",
    "        self.results_dir = './results/'\n",
    "        self.serial_batches = False\n",
    "        self.stylegan2_G_num_downsampling = 1\n",
    "        self.suffix = ''\n",
    "        self.verbose = False\n",
    "\n",
    "opt = Options()\n",
    "#dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "model = create_model(opt)      # create a model given opt.model and other options\n",
    "model.load_networks('latest')\n",
    "# create a webpage for viewing the results\n",
    "#web_dir = os.path.join(opt.results_dir, opt.name, '{}_{}'.format(opt.phase, opt.epoch))  # define the website directory\n",
    "#print('creating web directory', web_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tensor to image for displaying\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.clone().detach().cpu()  # Clone the tensor to avoid modifying the original and move to CPU\n",
    "    tensor = F.interpolate(tensor, size=[256,128], mode='bilinear', align_corners=False)\n",
    "    tensor = tensor.squeeze(0)  # Remove batch dimension\n",
    "    tensor = tensor * torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1) + torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)  # De-normalize\n",
    "    tensor = tensor.permute(1, 2, 0)  # Change from CxHxW to HxWxC\n",
    "    tensor = tensor.numpy()  # Convert to numpy array\n",
    "    tensor = (tensor * 255).astype(np.uint8)  # Convert to uint8\n",
    "    return tensor\n",
    "\n",
    "crop_size_w = 256\n",
    "crop_size_h = 256\n",
    "# Dataset loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((crop_size_h, crop_size_w)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market1501 dataset dir\n",
    "raw_data_dir_market1501 = Path('/home/jun/ReID_Dataset/Market-1501-v15.09.15')\n",
    "# Make dictionary of all image market1501 with keys of image_name and values of image_path\n",
    "market_train_dic = {i.name:i for i in sorted(list(raw_data_dir_market1501.glob('bounding_box_train/*.jpg')))}\n",
    "market_gallery_dic = {i.name:i for i in sorted(list(raw_data_dir_market1501.glob('bounding_box_test/*.jpg')))}\n",
    "market_query_dic = {i.name:i for i in sorted(list(raw_data_dir_market1501.glob('query/*.jpg')))}\n",
    "\n",
    "new_data_dir_market1501 = Path('/home/jun/ReID_Dataset/Market-1501-v15.09.15-CUT-copilot')\n",
    "new_data_dir_market1501.mkdir(exist_ok=True)\n",
    "new_market_train_dir = new_data_dir_market1501 / 'bounding_box_train'\n",
    "new_market_train_dir.mkdir(exist_ok=True)\n",
    "new_market_gallery_dir = new_data_dir_market1501 / 'bounding_box_test'\n",
    "new_market_gallery_dir.mkdir(exist_ok=True)\n",
    "new_market_query_dir = new_data_dir_market1501 / 'query'\n",
    "new_market_query_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for image_name, image_path in market_train_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_market_train_dir / image_name\n",
    "    output_image.save(output_image_path)\n",
    "\n",
    "for image_name, image_path in market_gallery_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_market_gallery_dir / image_name\n",
    "    output_image.save(output_image_path)\n",
    "    \n",
    "for image_name, image_path in market_query_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_market_query_dir / image_name\n",
    "    output_image.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model [CUTModel] was created\n",
      "loading the model from ./checkpoints/duke2market/latest_net_G.pth\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import save_images\n",
    "from util import html\n",
    "import util.util as util\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import models\n",
    "import data\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.CUT_mode = 'CUT'\n",
    "        self.batch_size = 1\n",
    "        self.checkpoints_dir = './checkpoints'\n",
    "        self.crop_size = 256\n",
    "        self.dataroot = '/home/jun/CUT/datasets/duke2market'\n",
    "        self.dataset_mode = 'unaligned'\n",
    "        self.direction = 'AtoB'\n",
    "        self.display_winsize = 256\n",
    "        self.easy_label = 'duke2market'\n",
    "        self.epoch = 'latest'\n",
    "        self.eval = False\n",
    "        self.flip_equivariance = False\n",
    "        self.gpu_ids = [0]\n",
    "        self.init_gain = 0.02\n",
    "        self.init_type = 'xavier'\n",
    "        self.input_nc = 3\n",
    "        self.isTrain = False\n",
    "        self.lambda_GAN = 1.0\n",
    "        self.lambda_NCE = 1.0\n",
    "        self.load_size = 256\n",
    "        self.max_dataset_size = float(\"inf\")\n",
    "        self.model = 'cut'\n",
    "        self.n_layers_D = 3\n",
    "        self.name = 'duke2market'\n",
    "        self.nce_T = 0.07\n",
    "        self.nce_idt = True\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.nce_layers = '0,4,8,12,16'\n",
    "        self.ndf = 64\n",
    "        self.netD = 'basic'\n",
    "        self.netF = 'mlp_sample'\n",
    "        self.netF_nc = 256\n",
    "        self.netG = 'resnet_9blocks'\n",
    "        self.ngf = 64\n",
    "        self.no_antialias = False\n",
    "        self.no_antialias_up = False\n",
    "        self.no_dropout = True\n",
    "        self.no_flip = False\n",
    "        self.normD = 'instance'\n",
    "        self.normG = 'instance'\n",
    "        self.num_patches = 256\n",
    "        self.num_test = 50\n",
    "        self.num_threads = 4\n",
    "        self.output_nc = 3\n",
    "        self.phase = 'test'\n",
    "        self.pool_size = 0\n",
    "        self.preprocess = 'resize_and_crop'\n",
    "        self.random_scale_max = 3.0\n",
    "        self.results_dir = './results/'\n",
    "        self.serial_batches = False\n",
    "        self.stylegan2_G_num_downsampling = 1\n",
    "        self.suffix = ''\n",
    "        self.verbose = False\n",
    "\n",
    "opt = Options()\n",
    "\n",
    "model = create_model(opt)      # create a model given opt.model and other options\n",
    "model.load_networks('latest')\n",
    "\n",
    "\n",
    "# duke1501 dataset dir\n",
    "raw_data_dir_duke = Path('/home/jun/ReID_Dataset/DukeMTMC-reID')\n",
    "# Make dictionary of all image duke1501 with keys of image_name and values of image_path\n",
    "duke_train_dic = {i.name:i for i in sorted(list(raw_data_dir_duke.glob('bounding_box_train/*.jpg')))}\n",
    "duke_gallery_dic = {i.name:i for i in sorted(list(raw_data_dir_duke.glob('bounding_box_test/*.jpg')))}\n",
    "duke_query_dic = {i.name:i for i in sorted(list(raw_data_dir_duke.glob('query/*.jpg')))}\n",
    "\n",
    "new_data_dir_duke = Path('/home/jun/ReID_Dataset/DukeMTMC-reID-marketstyle-CUT')\n",
    "new_data_dir_duke.mkdir(exist_ok=True)\n",
    "new_duke_train_dir = new_data_dir_duke / 'bounding_box_train'\n",
    "new_duke_train_dir.mkdir(exist_ok=True)\n",
    "new_duke_gallery_dir = new_data_dir_duke / 'bounding_box_test'\n",
    "new_duke_gallery_dir.mkdir(exist_ok=True)\n",
    "new_duke_query_dir = new_data_dir_duke / 'query'\n",
    "new_duke_query_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for image_name, image_path in duke_train_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_duke_train_dir / image_name\n",
    "    output_image.save(output_image_path)\n",
    "\n",
    "for image_name, image_path in duke_gallery_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_duke_gallery_dir / image_name\n",
    "    output_image.save(output_image_path)\n",
    "    \n",
    "for image_name, image_path in duke_query_dic.items():\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transformed_image = transform(image)\n",
    "    x = transformed_image.unsqueeze(0).cuda()\n",
    "    data = {'A':x,'B':x,\n",
    "            'A_paths': image_path,\n",
    "            'B_paths': image_path}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    # Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "    output_image = tensor_to_image(visuals['fake_B'])\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image_path = new_duke_query_dir / image_name\n",
    "    output_image.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'market_train_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_name, image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mmarket_train_dic\u001b[49m\u001b[38;5;241m.\u001b[39mitems())[\u001b[38;5;241m201\u001b[39m]\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m transform(image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'market_train_dic' is not defined"
     ]
    }
   ],
   "source": [
    "image_name, image_path = list(market_train_dic.items())[201]\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "transformed_image = transform(image)\n",
    "x = transformed_image.unsqueeze(0).cuda()\n",
    "data = {'A':x,'B':x,\n",
    "        'A_paths': image_path,\n",
    "        'B_paths': image_path}\n",
    "model.set_input(data)\n",
    "model.test()\n",
    "visuals = model.get_current_visuals()\n",
    "# Assuming `data` is the dictionary containing the tensors\n",
    "\n",
    "real_A_img = tensor_to_image(visuals['real_A'])\n",
    "fake_B_img = tensor_to_image(visuals['fake_B'])\n",
    "real_B_img = tensor_to_image(visuals['real_B'])\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Real A\")\n",
    "plt.imshow(real_A_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Fake B\")\n",
    "plt.imshow(fake_B_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Real B\")\n",
    "plt.imshow(real_B_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
