{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from UDAsbs import datasets\n",
    "from UDAsbs import models\n",
    "from UDAsbs.trainers import PreTrainer, PreTrainer_multi\n",
    "from UDAsbs.evaluators import Evaluator\n",
    "from UDAsbs.utils.data import IterLoader\n",
    "from UDAsbs.utils.data import transforms as T\n",
    "from UDAsbs.utils.data.sampler import RandomMultipleGallerySampler\n",
    "from UDAsbs.utils.data.preprocessor import Preprocessor\n",
    "from UDAsbs.utils.logging import Logger\n",
    "from UDAsbs.utils.serialization import load_checkpoint, save_checkpoint, copy_state_dict\n",
    "from UDAsbs.utils.lr_scheduler import WarmupMultiStepLR\n",
    "\n",
    "\n",
    "start_epoch = best_mAP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name, data_dir, height, width, batch_size, workers, num_instances, iters=200):\n",
    "    root = osp.join(data_dir)\n",
    "\n",
    "    dataset = datasets.create(name, root)\n",
    "\n",
    "    normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_set = dataset.train\n",
    "    num_classes = dataset.num_train_pids\n",
    "\n",
    "    train_transformer = T.Compose([\n",
    "             T.Resize((height, width), interpolation=3),\n",
    "             T.RandomHorizontalFlip(p=0.5),\n",
    "             T.Pad(10),\n",
    "             T.RandomCrop((height, width)),\n",
    "             # T.AugMix(),\n",
    "             T.ToTensor(),\n",
    "             normalizer\n",
    "         ])\n",
    "\n",
    "\n",
    "    test_transformer = T.Compose([\n",
    "             T.Resize((height, width), interpolation=3),\n",
    "             T.ToTensor(),\n",
    "             normalizer\n",
    "         ])\n",
    "\n",
    "    rmgs_flag = num_instances > 0\n",
    "    if rmgs_flag:\n",
    "        sampler = RandomMultipleGallerySampler(train_set, num_instances)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    train_loader = IterLoader(\n",
    "                DataLoader(Preprocessor(train_set, root=dataset.images_dir,\n",
    "                                        transform=train_transformer),\n",
    "                            batch_size=batch_size, num_workers=workers, sampler=sampler,\n",
    "                            shuffle=not rmgs_flag, pin_memory=True, drop_last=True), length=iters)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n",
    "                     root=dataset.images_dir, transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "    return dataset, num_classes, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Pre-training on the source domain\")\n",
    "# data\n",
    "parser.add_argument('-ds', '--dataset-source', type=str, default='market1501',\n",
    "                    choices=datasets.names())\n",
    "parser.add_argument('-dt', '--dataset-target', type=str, default='dukemtmc',\n",
    "                    choices=datasets.names())\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=64)\n",
    "parser.add_argument('-j', '--workers', type=int, default=4)\n",
    "parser.add_argument('--height', type=int, default=256, help=\"input height\")\n",
    "parser.add_argument('--width', type=int, default=128, help=\"input width\")\n",
    "parser.add_argument('--num-instances', type=int, default=4,\n",
    "                    help=\"each minibatch consist of \"\n",
    "                         \"(batch_size // num_instances) identities, and \"\n",
    "                         \"each identity has num_instances instances, \"\n",
    "                         \"default: 0 (NOT USE)\")\n",
    "# model\n",
    "parser.add_argument('-a', '--arch', type=str, default='resnet50',\n",
    "                    choices=models.names())\n",
    "parser.add_argument('--features', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0)\n",
    "# optimizer\n",
    "parser.add_argument('--lr', type=float, default=0.00035,\n",
    "                    help=\"learning rate of new parameters, for pretrained \")\n",
    "parser.add_argument('--momentum', type=float, default=0.9)\n",
    "parser.add_argument('--weight-decay', type=float, default=5e-4)\n",
    "parser.add_argument('--warmup-step', type=int, default=10)\n",
    "parser.add_argument('--milestones', nargs='+', type=int, default=[40, 70], help='milestones for the learning rate decay')\n",
    "# training configs\n",
    "parser.add_argument('--resume', type=str, default=\"\", metavar='PATH')\n",
    "#logs/market1501TOdukemtmc/resnet50-pretrain-1_gempooling/model_best.pth.tar\n",
    "parser.add_argument('--evaluate', action='store_true',\n",
    "                    help=\"evaluation only\")\n",
    "parser.add_argument('--eval-step', type=int, default=40)\n",
    "parser.add_argument('--rerank', action='store_true',\n",
    "                    help=\"evaluation only\")\n",
    "parser.add_argument('--epochs', type=int, default=80)\n",
    "parser.add_argument('--iters', type=int, default=200)\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--print-freq', type=int, default=100)\n",
    "parser.add_argument('--margin', type=float, default=0.0, help='margin for the triplet loss with batch hard')\n",
    "# path\n",
    "working_dir = osp.dirname(osp.abspath(''))\n",
    "parser.add_argument('--data-dir', type=str, metavar='PATH',\n",
    "                    default=osp.join(working_dir, '/home/jun/ReID_Dataset/'))\n",
    "parser.add_argument('--logs-dir', type=str, metavar='PATH',\n",
    "                    default=osp.join(working_dir, 'logs/demo'))\n",
    "\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Args:Namespace(dataset_source='market1501', dataset_target='dukemtmc', batch_size=64, workers=4, height=256, width=128, num_instances=4, arch='resnet50', features=0, dropout=0, lr=0.00035, momentum=0.9, weight_decay=0.0005, warmup_step=10, milestones=[40, 70], resume='', evaluate=False, eval_step=40, rerank=False, epochs=80, iters=200, seed=1, print_freq=100, margin=0.0, data_dir='/home/jun/ReID_Dataset/', logs_dir='/home/jun/logs/demo')\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "global start_epoch, best_mAP\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if not args.evaluate:\n",
    "    sys.stdout = Logger(osp.join(args.logs_dir, 'log.txt'))\n",
    "else:\n",
    "    log_dir = osp.dirname(args.resume)\n",
    "    sys.stdout = Logger(osp.join(log_dir, 'log_test.txt'))\n",
    "print(\"==========\\nArgs:{}\\n==========\".format(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "iters = args.iters if (args.iters>0) else None\n",
    "iters = None\n",
    "dataset_source, num_classes, train_loader_source, test_loader_source = \\\n",
    "    get_data(args.dataset_source, args.data_dir, args.height,\n",
    "                args.width, args.batch_size, args.workers, args.num_instances, iters)\n",
    "\n",
    "dataset_target, _, train_loader_target, test_loader_target = \\\n",
    "    get_data(args.dataset_target, args.data_dir, args.height,\n",
    "                args.width, args.batch_size, args.workers, 0, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f'Creating {args.arch} model with num_features = {args.features}, dropout = {args.dropout}, num_classes = {[num_classes]}')\n",
    "model = models.create(args.arch, num_features=args.features, dropout=args.dropout,\n",
    "                      num_classes=[num_classes])\n",
    "model.cuda()\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "input = Variable(torch.FloatTensor(32, 3, 256, 128)).cuda()\n",
    "model.train()\n",
    "output = model(input, training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2048]), torch.Size([32, 751]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint\n",
    "if args.resume:\n",
    "    checkpoint = load_checkpoint(args.resume)\n",
    "    copy_state_dict(checkpoint['state_dict'], model)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_mAP = checkpoint['best_mAP']\n",
    "    print(\"=> Start epoch {}  best mAP {:.1%}\"\n",
    "          .format(start_epoch, best_mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "evaluator = Evaluator(model)\n",
    "# args.evaluate=True\n",
    "if args.evaluate:\n",
    "    print(\"Test on source domain:\")\n",
    "    evaluator.evaluate(test_loader_source, dataset_source.query, dataset_source.gallery, cmc_flag=True, rerank=args.rerank)\n",
    "    print(\"Test on target domain:\")\n",
    "    evaluator.evaluate(test_loader_target, dataset_target.query, dataset_target.gallery, cmc_flag=True, rerank=args.rerank)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for key, value in model.named_parameters():\n",
    "    if not value.requires_grad:\n",
    "        continue\n",
    "    params += [{\"params\": [value], \"lr\": args.lr, \"weight_decay\": args.weight_decay}]\n",
    "optimizer = torch.optim.Adam(params)\n",
    "lr_scheduler = WarmupMultiStepLR(optimizer, args.milestones, gamma=0.1, warmup_factor=0.01,\n",
    "                                 warmup_iters=args.warmup_step)\n",
    "\n",
    "# Trainer\n",
    "trainer = PreTrainer(model, num_classes, margin=args.margin) if 'multi' not in args.arch else PreTrainer_multi(model, num_classes, margin=args.margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loader_source\u001b[38;5;241m.\u001b[39mnew_epoch()\n\u001b[1;32m      4\u001b[0m train_loader_target\u001b[38;5;241m.\u001b[39mnew_epoch()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39margs\u001b[38;5;241m.\u001b[39meval_step\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (epoch\u001b[38;5;241m==\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m~/UDAStrongBaseline/UDAsbs/trainers.py:143\u001b[0m, in \u001b[0;36mPreTrainer.train\u001b[0;34m(self, epoch, data_loader_source, data_loader_target, optimizer, train_iters, print_freq)\u001b[0m\n\u001b[1;32m    140\u001b[0m losses_tr\u001b[38;5;241m.\u001b[39mupdate(loss_tr\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    141\u001b[0m precisions\u001b[38;5;241m.\u001b[39mupdate(prec1)\n\u001b[0;32m--> 143\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/optimizer.py:825\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 825\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    train_loader_source.new_epoch()\n",
    "    train_loader_target.new_epoch()\n",
    "\n",
    "    trainer.train(epoch, train_loader_source, train_loader_target, optimizer,\n",
    "                train_iters=len(train_loader_source), print_freq=args.print_freq)\n",
    "    lr_scheduler.step()\n",
    "    if ((epoch+1)%args.eval_step==0 or (epoch==args.epochs-1)):\n",
    "\n",
    "        _, mAP = evaluator.evaluate(test_loader_source, dataset_source.query,\n",
    "                                    dataset_source.gallery, cmc_flag=True)\n",
    "\n",
    "        is_best = mAP > best_mAP\n",
    "        best_mAP = max(mAP, best_mAP)\n",
    "        save_checkpoint({\n",
    "            'state_dict': model.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_mAP': best_mAP,\n",
    "        }, is_best, fpath=osp.join(args.logs_dir, 'checkpoint.pth.tar'))\n",
    "\n",
    "        print('\\n * Finished epoch {:3d}  source mAP: {:5.1%}  best: {:5.1%}{}\\n'.\n",
    "                format(epoch, mAP, best_mAP, ' *' if is_best else ''))\n",
    "print(\"Test on source domain:\")\n",
    "evaluator.evaluate(test_loader_source, dataset_source.query, dataset_source.gallery, cmc_flag=True, rerank=args.rerank)\n",
    "print(\"Test on target domain:\")\n",
    "evaluator.evaluate(test_loader_target, dataset_target.query, dataset_target.gallery, cmc_flag=True, rerank=args.rerank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
