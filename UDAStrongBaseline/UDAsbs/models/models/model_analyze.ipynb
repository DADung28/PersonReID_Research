{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/miniconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.nn import knn\n",
    "from net.pspnet import PSPNet\n",
    "from hrnet.seg_hrnet import HighResolutionNet, HRNet_2Head\n",
    "import pretrainedmodels\n",
    "import timm\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from typing import Optional\n",
    "from gem_pooling import GeneralizedMeanPoolingP\n",
    "#from .backbones.resnet import BasicBlock, Bottleneck, ResNet\n",
    "#from .backbones.resnet_ibn_a import resnet50_ibn_a, resnet101_ibn_a\n",
    "\n",
    "proxy = 'http://10.0.0.107:3128'\n",
    "os.environ['http_proxy'] = proxy \n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin(nn.Module):\n",
    "    def __init__(self, mb_h=2048, with_nl=False,pretrained=True, cut_at_pooling=False,\n",
    "                 num_features=0, norm=False, dropout=0, num_classes=None, sour_class=751):\n",
    "        super().__init__()\n",
    "            \n",
    "        model_ft = timm.create_model('swin_base_patch4_window7_224', pretrained=True, drop_path_rate = 0.2)\n",
    "        # avg pooling to global pooling\n",
    "        #model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        model_ft.head = nn.Sequential() # save memory\n",
    "        self.model = model_ft\n",
    "        self.cut_at_pooling = cut_at_pooling\n",
    "        print(\"GeneralizedMeanPoolingP\")\n",
    "        self.gap = GeneralizedMeanPoolingP(3)\n",
    "        self.memorybank_fc = nn.Linear(mb_h, mb_h)\n",
    "        self.mbn=nn.BatchNorm1d(mb_h)\n",
    "        init.kaiming_normal_(self.memorybank_fc.weight, mode='fan_out')\n",
    "        init.constant_(self.memorybank_fc.bias, 0)\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.has_embedding = num_features > 0\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        out_planes = model_ft.norm.normalized_shape[0]\n",
    "\n",
    "        # Append new layers\n",
    "        if self.has_embedding:\n",
    "            self.feat = nn.Linear(out_planes, self.num_features)\n",
    "            self.feat_bn = nn.BatchNorm1d(self.num_features)\n",
    "            init.kaiming_normal_(self.feat.weight, mode='fan_out')\n",
    "            init.constant_(self.feat.bias, 0)\n",
    "        else:\n",
    "            # Change the num_features to CNN output channels\n",
    "            self.num_features = out_planes\n",
    "            self.feat_bn = nn.BatchNorm1d(self.num_features)\n",
    "        self.feat_bn.bias.requires_grad_(False)\n",
    "        if self.dropout > 0:\n",
    "            self.drop = nn.Dropout(self.dropout)\n",
    "        if self.num_classes is not None:\n",
    "            for i,num_cluster in enumerate(self.num_classes):\n",
    "                exec(\"self.classifier{}_{} = nn.Linear(self.num_features, {}, bias=False)\".format(i,num_cluster,num_cluster))\n",
    "                exec(\"init.normal_(self.classifier{}_{}.weight, std=0.001)\".format(i,num_cluster))\n",
    "    def forward(self, x, feature_withbn=False, training=False):\n",
    "        x = self.model.forward_features(x) # [batchsize, 7, 7, 1024]\n",
    "        \n",
    "        x = x.permute(0,3,1,2) # Change shape from [batchsize, 7, 7, 1024] -> [batchsize, 49, 1024]\n",
    "        # swin is update in latest timm>0.6.0, so I add the following two lines.\n",
    "        \n",
    "        x = self.gap(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.cut_at_pooling:return x#FALSE\n",
    "\n",
    "        if self.has_embedding:\n",
    "            bn_x = self.feat_bn(self.feat(x))#FALSE\n",
    "        else:\n",
    "            bn_x = self.feat_bn(x)#1\n",
    "\n",
    "        if training is False:\n",
    "            bn_x = F.normalize(bn_x)\n",
    "            return bn_x\n",
    "\n",
    "        if self.norm:#FALSE\n",
    "            bn_x = F.normalize(bn_x)\n",
    "        elif self.has_embedding:#FALSE\n",
    "            bn_x = F.relu(bn_x)\n",
    "\n",
    "        if self.dropout > 0:#FALSE\n",
    "            bn_x = self.drop(bn_x)\n",
    "\n",
    "        prob = []\n",
    "        if self.num_classes is not None:\n",
    "            for i,num_cluster in enumerate(self.num_classes):\n",
    "                exec(\"prob.append(self.classifier{}_{}(bn_x))\".format(i,num_cluster))\n",
    "        else:\n",
    "            return x, bn_x\n",
    "\n",
    "        if feature_withbn:#False\n",
    "           return bn_x, prob\n",
    "        mb_x = self.mbn(self.memorybank_fc(bn_x))\n",
    "        return x, prob, mb_x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/miniconda3/envs/py312/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jun/miniconda3/envs/py312/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneralizedMeanPoolingP\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, mb_h=2048, with_nl=False,pretrained=True, cut_at_pooling=False,\n",
    "                 num_features=0, norm=False, dropout=0, num_classes=None, sour_class=751):\n",
    "        super().__init__()\n",
    "        model_ft = models.resnet50(pretrained=pretrained)\n",
    "        model_ft.layer4[0].downsample[0].stride = (1,1)\n",
    "        model_ft.layer4[0].conv2.stride = (1,1)\n",
    "        self.model = model_ft\n",
    "        \n",
    "        self.cut_at_pooling = cut_at_pooling\n",
    "        print(\"GeneralizedMeanPoolingP\")\n",
    "        self.gap = GeneralizedMeanPoolingP(3)\n",
    "        self.memorybank_fc = nn.Linear(2048, mb_h)\n",
    "        self.mbn=nn.BatchNorm1d(mb_h)\n",
    "        init.kaiming_normal_(self.memorybank_fc.weight, mode='fan_out')\n",
    "        init.constant_(self.memorybank_fc.bias, 0)\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.has_embedding = num_features > 0\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        out_planes = model_ft.fc.in_features\n",
    "\n",
    "        # Append new layers\n",
    "        if self.has_embedding:\n",
    "            self.feat = nn.Linear(out_planes, self.num_features)\n",
    "            self.feat_bn = nn.BatchNorm1d(self.num_features)\n",
    "            init.kaiming_normal_(self.feat.weight, mode='fan_out')\n",
    "            init.constant_(self.feat.bias, 0)\n",
    "        else:\n",
    "            # Change the num_features to CNN output channels\n",
    "            self.num_features = out_planes\n",
    "            self.feat_bn = nn.BatchNorm1d(self.num_features)\n",
    "        self.feat_bn.bias.requires_grad_(False)\n",
    "        if self.dropout > 0:\n",
    "            self.drop = nn.Dropout(self.dropout)\n",
    "        if self.num_classes is not None:\n",
    "            for i,num_cluster in enumerate(self.num_classes):\n",
    "                exec(\"self.classifier{}_{} = nn.Linear(self.num_features, {}, bias=False)\".format(i,num_cluster,num_cluster))\n",
    "                exec(\"init.normal_(self.classifier{}_{}.weight, std=0.001)\".format(i,num_cluster))\n",
    "    def forward(self, x, feature_withbn=False, training=False):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.gap(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.cut_at_pooling:return x#FALSE\n",
    "\n",
    "        if self.has_embedding:\n",
    "            bn_x = self.feat_bn(self.feat(x))#FALSE\n",
    "        else:\n",
    "            bn_x = self.feat_bn(x)#1\n",
    "\n",
    "        if training is False:\n",
    "            bn_x = F.normalize(bn_x)\n",
    "            return bn_x\n",
    "\n",
    "        if self.norm:#FALSE\n",
    "            bn_x = F.normalize(bn_x)\n",
    "        elif self.has_embedding:#FALSE\n",
    "            bn_x = F.relu(bn_x)\n",
    "\n",
    "        if self.dropout > 0:#FALSE\n",
    "            bn_x = self.drop(bn_x)\n",
    "\n",
    "        prob = []\n",
    "        if self.num_classes is not None:\n",
    "            for i,num_cluster in enumerate(self.num_classes):\n",
    "                exec(\"prob.append(self.classifier{}_{}(bn_x))\".format(i,num_cluster))\n",
    "        else:\n",
    "            return x, bn_x\n",
    "\n",
    "        if feature_withbn:#False\n",
    "           return bn_x, prob\n",
    "        mb_x = self.mbn(self.memorybank_fc(bn_x))\n",
    "        return x, prob, mb_x, None\n",
    "\n",
    "model = ResNet50()  \n",
    "input = torch.rand((32,3,256,128))\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneralizedMeanPoolingP\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "mb_h = 1024 # Number of features 's channel\n",
    "sour_class = 751 # Number of source dataset classes\n",
    "num_features = 0 # Number of features if we want to add an LN layers\n",
    "dropout = 0 # Dropout propability of dropout layer\n",
    "ncs = [int(x) for x in '60'.split(',')]\n",
    "fc_len = 3500\n",
    "num_classes = [fc_len for _ in range(len(ncs))]\n",
    "model = Swin(mb_h=mb_h, with_nl=False,pretrained=True, cut_at_pooling=False,\n",
    "                 num_features=num_features, norm=False, dropout=dropout, num_classes=num_classes, sour_class=sour_class).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((32,3,224,224)).to('cpu')\n",
    "output = model(input, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1024]), torch.Size([32, 3500]), torch.Size([32, 1024]), None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1][0].shape, output[2].shape, output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = timm.create_model('swin_base_patch4_window7_224', pretrained=True, drop_path_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.norm.normalized_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "SwinTransformer (SwinTransformer)                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
       "├─PatchEmbed (patch_embed)                         [32, 3, 224, 224]    [32, 56, 56, 128]    --                   True\n",
       "│    └─Conv2d (proj)                               [32, 3, 224, 224]    [32, 128, 56, 56]    6,272                True\n",
       "│    └─LayerNorm (norm)                            [32, 56, 56, 128]    [32, 56, 56, 128]    256                  True\n",
       "├─Sequential (layers)                              [32, 56, 56, 128]    [32, 7, 7, 1024]     --                   True\n",
       "│    └─SwinTransformerStage (0)                    [32, 56, 56, 128]    [32, 56, 56, 128]    --                   True\n",
       "│    │    └─Identity (downsample)                  [32, 56, 56, 128]    [32, 56, 56, 128]    --                   --\n",
       "│    │    └─Sequential (blocks)                    [32, 56, 56, 128]    [32, 56, 56, 128]    397,896              True\n",
       "│    └─SwinTransformerStage (1)                    [32, 56, 56, 128]    [32, 28, 28, 256]    --                   True\n",
       "│    │    └─PatchMerging (downsample)              [32, 56, 56, 128]    [32, 28, 28, 256]    132,096              True\n",
       "│    │    └─Sequential (blocks)                    [32, 28, 28, 256]    [32, 28, 28, 256]    1,582,224            True\n",
       "│    └─SwinTransformerStage (2)                    [32, 28, 28, 256]    [32, 14, 14, 512]    --                   True\n",
       "│    │    └─PatchMerging (downsample)              [32, 28, 28, 256]    [32, 14, 14, 512]    526,336              True\n",
       "│    │    └─Sequential (blocks)                    [32, 14, 14, 512]    [32, 14, 14, 512]    56,791,584           True\n",
       "│    └─SwinTransformerStage (3)                    [32, 14, 14, 512]    [32, 7, 7, 1024]     --                   True\n",
       "│    │    └─PatchMerging (downsample)              [32, 14, 14, 512]    [32, 7, 7, 1024]     2,101,248            True\n",
       "│    │    └─Sequential (blocks)                    [32, 7, 7, 1024]     [32, 7, 7, 1024]     25,203,264           True\n",
       "├─LayerNorm (norm)                                 [32, 7, 7, 1024]     [32, 7, 7, 1024]     2,048                True\n",
       "├─ClassifierHead (head)                            [32, 7, 7, 1024]     [32, 1000]           --                   True\n",
       "│    └─SelectAdaptivePool2d (global_pool)          [32, 7, 7, 1024]     [32, 1024]           --                   --\n",
       "│    │    └─FastAdaptiveAvgPool (pool)             [32, 7, 7, 1024]     [32, 1024]           --                   --\n",
       "│    │    └─Identity (flatten)                     [32, 1024]           [32, 1024]           --                   --\n",
       "│    └─Dropout (drop)                              [32, 1024]           [32, 1024]           --                   --\n",
       "│    └─Linear (fc)                                 [32, 1024]           [32, 1000]           1,025,000            True\n",
       "│    └─Identity (flatten)                          [32, 1000]           [32, 1000]           --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 87,768,224\n",
       "Trainable params: 87,768,224\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.77\n",
       "==================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 9248.70\n",
       "Params size (MB): 350.82\n",
       "Estimated Total Size (MB): 9618.78\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model_ft, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "       row_settings=[\"var_names\"]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
